{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e6f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bf5ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5281f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47a3758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c435600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\awscl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\awscl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\awscl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\awscl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources if not already present\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download the stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the English stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7098113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    # Decontracting words\n",
    "    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "    # https://stackoverflow.com/a/19794953\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52205c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    \n",
    "    x=[word for word in df['review'][i].split() if word not in english_stopwords]\n",
    "    y=\" \".join(x)\n",
    "    df['review'][i]=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe37b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"one reviewers mentioned watching 1 oz episode hooked. right, exactly happened me.<br /><br />the first thing struck oz brutality unflinching scenes violence, set right word go. trust me, show faint hearted timid. show pulls punches regards drugs, sex violence. hardcore, classic use word.<br /><br />it called oz nickname given oswald maximum security state penitentary. focuses mainly emerald city, experimental section prison cells glass fronts face inwards, privacy high agenda. em city home many..aryans, muslims, gangstas, latinos, christians, italians, irish more....so scuffles, death stares, dodgy dealings shady agreements never far away.<br /><br />i would say main appeal show due fact goes shows dare. forget pretty pictures painted mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw struck nasty surreal, say ready it, watched more, developed taste oz, got accustomed high levels graphic violence. violence, injustice (crooked guards who'll sold nickel, inmates who'll kill order get away it, well mannered, middle class inmates turned prison bitches due lack street skills prison experience) watching oz, may become comfortable uncomfortable viewing....thats get touch darker side.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a41c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    \n",
    "    x=[lemmatizer.lemmatize(word) for word in df['review'][i].split()]\n",
    "    y=\" \".join(x)\n",
    "    df['review'][i]=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9be24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"one reviewer mentioned watching 1 oz episode hooked. right, exactly happened me.<br /><br />the first thing struck oz brutality unflinching scene violence, set right word go. trust me, show faint hearted timid. show pull punch regard drugs, sex violence. hardcore, classic use word.<br /><br />it called oz nickname given oswald maximum security state penitentary. focus mainly emerald city, experimental section prison cell glass front face inwards, privacy high agenda. em city home many..aryans, muslims, gangstas, latinos, christians, italians, irish more....so scuffles, death stares, dodgy dealing shady agreement never far away.<br /><br />i would say main appeal show due fact go show dare. forget pretty picture painted mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw struck nasty surreal, say ready it, watched more, developed taste oz, got accustomed high level graphic violence. violence, injustice (crooked guard who'll sold nickel, inmate who'll kill order get away it, well mannered, middle class inmate turned prison bitch due lack street skill prison experience) watching oz, may become comfortable uncomfortable viewing....thats get touch darker side.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08f702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5517dadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching 1 oz episode hooked  right  exactly happened me the first thing struck oz brutality unflinching scene violence  set right word go  trust me  show faint hearted timid  show pull punch regard drugs  sex violence  hardcore  classic use word it called oz nickname given oswald maximum security state penitentary  focus mainly emerald city  experimental section prison cell glass front face inwards  privacy high agenda  em city home many  aryans  muslims  gangstas  latinos  christians  italians  irish more    so scuffles  death stares  dodgy dealing shady agreement never far away i would say main appeal show due fact go show dare  forget pretty picture painted mainstream audiences  forget charm  forget romance   oz mess around  first episode ever saw struck nasty surreal  say ready it  watched more  developed taste oz  got accustomed high level graphic violence  violence  injustice  crooked guard who will sold nickel  inmate who will kill order get away it  well mannered  middle class inmate turned prison bitch due lack street skill prison experience  watching oz  may become comfortable uncomfortable viewing    thats get touch darker side'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3510605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d8c04f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>1950</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youth</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.087335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.110997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0.083815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             10  100   11   12   13   14   15   17   18  1950  ...  yet  \\\n",
       "0      0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "1      0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "2      0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "3      0.087335  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "4      0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...  ...   \n",
       "49995  0.083815  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "49996  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "49997  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "49998  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "49999  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  0.0   \n",
       "\n",
       "           york  you    young  younger  yourself  youth  zero    zombie  \\\n",
       "0      0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "1      0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "2      0.000000  0.0  0.08171      0.0  0.000000    0.0   0.0  0.000000   \n",
       "3      0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.308333   \n",
       "4      0.110997  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "...         ...  ...      ...      ...       ...    ...   ...       ...   \n",
       "49995  0.000000  0.0  0.00000      0.0  0.154098    0.0   0.0  0.000000   \n",
       "49996  0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "49997  0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "49998  0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "49999  0.000000  0.0  0.00000      0.0  0.000000    0.0   0.0  0.000000   \n",
       "\n",
       "       zombies  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "...        ...  \n",
       "49995      0.0  \n",
       "49996      0.0  \n",
       "49997      0.0  \n",
       "49998      0.0  \n",
       "49999      0.0  \n",
       "\n",
       "[50000 rows x 3000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9369846e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production  the filming techn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there is family little boy  jake  th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s  love time money  visually stu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>thought movie right good job  creative origina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot  bad dialogue  bad acting  idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>catholic taught parochial elementary school nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i am going disagree previous comment side malt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>one expects star trek movie high art  fan expe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      one reviewer mentioned watching 1 oz episode h...          1\n",
       "1      wonderful little production  the filming techn...          1\n",
       "2      thought wonderful way spend time hot summer we...          1\n",
       "3      basically there is family little boy  jake  th...          0\n",
       "4      petter mattei s  love time money  visually stu...          1\n",
       "...                                                  ...        ...\n",
       "49995  thought movie right good job  creative origina...          1\n",
       "49996  bad plot  bad dialogue  bad acting  idiotic di...          0\n",
       "49997  catholic taught parochial elementary school nu...          0\n",
       "49998  i am going disagree previous comment side malt...          0\n",
       "49999  one expects star trek movie high art  fan expe...          0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-02 08:29:26,132] A new study created in memory with name: no-name-263ff3e8-6147-4570-9dd9-a439061503f6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94c155ddbbe45a1a62f0999abbca35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-03-02 08:37:58,297] Trial 0 finished with value: 0.8694 and parameters: {'learning_rate': 0.15486178499497188, 'max_depth': 5, 'subsample': 0.9388977400102805, 'colsample_bytree': 0.8885423312024768, 'n_estimators': 296}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 08:42:41,393] Trial 1 finished with value: 0.855 and parameters: {'learning_rate': 0.14915813890073298, 'max_depth': 4, 'subsample': 0.8311206942602125, 'colsample_bytree': 0.8900456051870822, 'n_estimators': 202}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 08:50:57,264] Trial 2 finished with value: 0.8367 and parameters: {'learning_rate': 0.05439979768136664, 'max_depth': 5, 'subsample': 0.874570688851952, 'colsample_bytree': 0.8008010936444638, 'n_estimators': 229}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 08:53:19,477] Trial 3 finished with value: 0.8304 and parameters: {'learning_rate': 0.13618332183169493, 'max_depth': 3, 'subsample': 0.8485230659912092, 'colsample_bytree': 0.9697449792545114, 'n_estimators': 128}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 08:58:45,827] Trial 4 finished with value: 0.8627 and parameters: {'learning_rate': 0.17877584895792628, 'max_depth': 5, 'subsample': 0.8705339554532644, 'colsample_bytree': 0.8488007301196656, 'n_estimators': 176}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 09:03:55,797] Trial 5 finished with value: 0.8162 and parameters: {'learning_rate': 0.05014373069487662, 'max_depth': 5, 'subsample': 0.9515040925642775, 'colsample_bytree': 0.846591786673003, 'n_estimators': 152}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 09:12:21,448] Trial 6 finished with value: 0.8564 and parameters: {'learning_rate': 0.09658116874872899, 'max_depth': 5, 'subsample': 0.8727716959745004, 'colsample_bytree': 0.8709319781444651, 'n_estimators': 266}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 09:23:47,816] Trial 7 finished with value: 0.7818 and parameters: {'learning_rate': 0.014312641474756544, 'max_depth': 5, 'subsample': 0.9659426363693417, 'colsample_bytree': 0.9883852440389942, 'n_estimators': 233}. Best is trial 0 with value: 0.8694.\n",
      "[I 2024-03-02 09:29:47,151] Trial 8 finished with value: 0.8717 and parameters: {'learning_rate': 0.19617940576995083, 'max_depth': 4, 'subsample': 0.8512856685644217, 'colsample_bytree': 0.9888999179098097, 'n_estimators': 277}. Best is trial 8 with value: 0.8717.\n",
      "[I 2024-03-02 09:33:07,685] Trial 9 finished with value: 0.8515 and parameters: {'learning_rate': 0.1495588696859861, 'max_depth': 3, 'subsample': 0.8517514417416177, 'colsample_bytree': 0.993440571305034, 'n_estimators': 213}. Best is trial 8 with value: 0.8717.\n",
      "[I 2024-03-02 09:38:52,261] Trial 10 finished with value: 0.8748 and parameters: {'learning_rate': 0.19476580004283103, 'max_depth': 4, 'subsample': 0.8102733703724494, 'colsample_bytree': 0.9394397503569483, 'n_estimators': 299}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 09:44:53,980] Trial 11 finished with value: 0.8747 and parameters: {'learning_rate': 0.19851191561714898, 'max_depth': 4, 'subsample': 0.8043250425536705, 'colsample_bytree': 0.9360610326036665, 'n_estimators': 296}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 09:51:19,652] Trial 12 finished with value: 0.8698 and parameters: {'learning_rate': 0.190025143712394, 'max_depth': 4, 'subsample': 0.8018589281873667, 'colsample_bytree': 0.9179541553400079, 'n_estimators': 256}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 09:58:32,192] Trial 13 finished with value: 0.8595 and parameters: {'learning_rate': 0.10541235421727684, 'max_depth': 4, 'subsample': 0.8001418108787323, 'colsample_bytree': 0.9358737363000882, 'n_estimators': 298}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:02:33,258] Trial 14 finished with value: 0.8592 and parameters: {'learning_rate': 0.17316769174892016, 'max_depth': 3, 'subsample': 0.9083751910533684, 'colsample_bytree': 0.9488254910536742, 'n_estimators': 250}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:07:59,364] Trial 15 finished with value: 0.8596 and parameters: {'learning_rate': 0.1139680865307937, 'max_depth': 4, 'subsample': 0.8179179149004512, 'colsample_bytree': 0.9466951821204032, 'n_estimators': 285}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:11:22,499] Trial 16 finished with value: 0.8606 and parameters: {'learning_rate': 0.19925556713771944, 'max_depth': 4, 'subsample': 0.9047544614135747, 'colsample_bytree': 0.9187681154099477, 'n_estimators': 178}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:13:12,136] Trial 17 finished with value: 0.8318 and parameters: {'learning_rate': 0.16792745121136687, 'max_depth': 3, 'subsample': 0.9908212846600996, 'colsample_bytree': 0.9637052236431694, 'n_estimators': 106}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:17:57,643] Trial 18 finished with value: 0.8548 and parameters: {'learning_rate': 0.1195497043611574, 'max_depth': 4, 'subsample': 0.8261681131919774, 'colsample_bytree': 0.9167555945686483, 'n_estimators': 240}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 10:23:23,826] Trial 19 finished with value: 0.8489 and parameters: {'learning_rate': 0.08544593852961625, 'max_depth': 4, 'subsample': 0.8189010887051879, 'colsample_bytree': 0.936965577072948, 'n_estimators': 271}. Best is trial 10 with value: 0.8748.\n",
      "[I 2024-03-02 11:01:17,389] Trial 20 finished with value: 0.8519 and parameters: {'learning_rate': 0.13301063877387326, 'max_depth': 3, 'subsample': 0.924712502507914, 'colsample_bytree': 0.9051440054691662, 'n_estimators': 287}. Best is trial 10 with value: 0.8748.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming tfidf_df is your feature matrix and df['sentiment'] is the target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Assuming you have a multi-class classification task\n",
    "        'eval_metric': 'mlogloss',     # Change this based on your task\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.8, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "        'num_class': len(df['sentiment'].unique())  # Set num_class based on the number of unique classes\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create the Optuna study and perform optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(\"Accuracy on Test Set:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e335c8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
